{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873015e8",
   "metadata": {},
   "source": [
    "# Using numerical and categorical variables together\n",
    "\n",
    "In the previous notebooks, we showed the required preprocessing to apply when\n",
    "dealing with numerical and categorical variables. However, we decoupled the\n",
    "process to treat each type individually. In this notebook, we show how to\n",
    "combine these preprocessing steps.\n",
    "\n",
    "We first load the entire adult census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c67328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")\n",
    "# drop the duplicated column `\"education-num\"` as stated in the first notebook\n",
    "adult_census = adult_census.drop(columns=\"education-num\")\n",
    "\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "\n",
    "data = adult_census.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c6b05",
   "metadata": {},
   "source": [
    "## Selection based on data types\n",
    "\n",
    "We separate categorical and numerical variables using their data types to\n",
    "identify them, as we saw previously that `object` corresponds to categorical\n",
    "columns (strings). We make use of `make_column_selector` helper to select the\n",
    "corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea11f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa35b8",
   "metadata": {},
   "source": [
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p>Here, we know that <tt class=\"docutils literal\">object</tt> data type is used to represent strings and thus\n",
    "categorical features. Be aware that this is not always the case. Sometimes\n",
    "<tt class=\"docutils literal\">object</tt> data type could contain other types of information, such as dates that\n",
    "were not properly formatted (strings) and yet relate to a quantity of\n",
    "elapsed  time.</p>\n",
    "<p class=\"last\">In a more general scenario you should manually introspect the content of your\n",
    "dataframe not to wrongly use <tt class=\"docutils literal\">make_column_selector</tt>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc561d19",
   "metadata": {},
   "source": [
    "## Dispatch columns to a specific processor\n",
    "\n",
    "In the previous sections, we saw that we need to treat data differently\n",
    "depending on their nature (i.e. numerical or categorical).\n",
    "\n",
    "Scikit-learn provides a `ColumnTransformer` class which sends specific\n",
    "columns to a specific transformer, making it easy to fit a single predictive\n",
    "model on a dataset that combines both kinds of variables together\n",
    "(heterogeneously typed tabular data).\n",
    "\n",
    "We first define the columns depending on their data type:\n",
    "\n",
    "* **one-hot encoding** is applied to categorical columns. Besides, we use\n",
    "  `handle_unknown=\"ignore\"` to solve the potential issues due to rare\n",
    "  categories.\n",
    "* **numerical scaling** numerical features which will be standardized.\n",
    "\n",
    "Now, we create our `ColumnTransfomer` by specifying three values: the\n",
    "preprocessor name, the transformer, and the columns. First, let's create the\n",
    "preprocessors for the numerical and categorical parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecc245",
   "metadata": {},
   "source": [
    "Now, we create the transformer and associate each of these preprocessors with\n",
    "their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"one-hot-encoder\", categorical_preprocessor, categorical_columns),\n",
    "        (\"standard_scaler\", numerical_preprocessor, numerical_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735635c8",
   "metadata": {},
   "source": [
    "We can take a minute to represent graphically the structure of a\n",
    "`ColumnTransformer`:\n",
    "\n",
    "![columntransformer diagram](../figures/api_diagram-columntransformer.svg)\n",
    "\n",
    "A `ColumnTransformer` does the following:\n",
    "\n",
    "* It **splits the columns** of the original dataset based on the column names\n",
    "  or indices provided. We obtain as many subsets as the number of transformers\n",
    "  passed into the `ColumnTransformer`.\n",
    "* It **transforms each subsets**. A specific transformer is applied to each\n",
    "  subset: it internally calls `fit_transform` or `transform`. The output of\n",
    "  this step is a set of transformed datasets.\n",
    "* It then **concatenates the transformed datasets** into a single dataset.\n",
    "\n",
    "The important thing is that `ColumnTransformer` is like any other scikit-learn\n",
    "transformer. In particular it can be combined with a classifier in a\n",
    "`Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ba4bd",
   "metadata": {},
   "source": [
    "The final model is more complex than the previous models but still follows the\n",
    "same API (the same set of methods that can be called by the user):\n",
    "\n",
    "- the `fit` method is called to preprocess the data and then train the\n",
    "  classifier of the preprocessed data;\n",
    "- the `predict` method makes predictions on new data;\n",
    "- the `score` method is used to predict on the test data and compare the\n",
    "  predictions to the expected test labels to compute the accuracy.\n",
    "\n",
    "Let's start by splitting our data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe99517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1f24b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Be aware that we use <tt class=\"docutils literal\">train_test_split</tt> here for didactic purposes, to show\n",
    "the scikit-learn API. In a real setting one might prefer to use\n",
    "cross-validation to also be able to evaluate the uncertainty of our estimation\n",
    "of the generalization performance of a model, as previously demonstrated.</p>\n",
    "</div>\n",
    "\n",
    "Now, we can train the model on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16363c0",
   "metadata": {},
   "source": [
    "Then, we can send the raw dataset straight to the pipeline. Indeed, we do not\n",
    "need to make any manual preprocessing (calling the `transform` or\n",
    "`fit_transform` methods) as it is already handled when calling the `predict`\n",
    "method. As an example, we predict on the five first samples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256ef48",
   "metadata": {},
   "source": [
    "To get directly the accuracy score, we need to call the `score` method. Let's\n",
    "compute the accuracy score on the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c6e91",
   "metadata": {},
   "source": [
    "## Evaluation of the model with cross-validation\n",
    "\n",
    "As previously stated, a predictive model should be evaluated by\n",
    "cross-validation. Our model is usable with the cross-validation tools of\n",
    "scikit-learn as any other predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01655ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc920eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(\n",
    "    \"The mean cross-validation accuracy is: \"\n",
    "    f\"{scores.mean():.3f} ± {scores.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80af99",
   "metadata": {},
   "source": [
    "The compound model has a higher predictive accuracy than the two models that\n",
    "used numerical and categorical variables in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a179b9",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "\n",
    "* used a `ColumnTransformer` to apply different preprocessing for categorical\n",
    "  and numerical variables;\n",
    "* used a pipeline to chain the `ColumnTransformer` preprocessing and logistic\n",
    "  regression fitting."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
